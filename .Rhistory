beta_s = rnorm(n = n, mean = mean_beta, sd = sd_beta)
cov.mat = sd_within^2 * ar1_gen(p = p, rho =  rho)
eps = mvrnorm(n = n, mu = rep(0,p), Sigma = cov.mat)
simu = data.frame()
for(i in 1:n){
y_ij = alpha_s[i] + beta_s[i] * t + c(t(eps[i,]))
simu = rbind(simu, y_ij)
}
simu = as.data.frame(simu)
names(simu) = paste0("M",t)
simu
}
# 2. Parameter set up -----------------------------------------------------
mean_alpha = 36
sd_alpha = 18
sd_within = 10
pct = 0.2
cv = 0.5
mean_beta_control = 1.5
sd_beta_control = cv * mean_beta_control
mean_beta_case = mean_beta_control * (1 - pct)
sd_beta_case = cv * mean_beta_case
n = 50
for(iter in c(1:1000)){
# print(iter)
simu_case = simulate_traditional(mean_alpha = mean_alpha, sd_alpha = sd_alpha, mean_beta = mean_beta_case, sd_within = sd_within,
sd_beta = sd_beta_case, n = n)
simu_control = simulate_traditional(mean_alpha = mean_alpha, sd_alpha = sd_alpha, mean_beta = mean_beta_control, sd_within = sd_within,
sd_beta = sd_beta_control, n = n)
beta_case = apply(simu_case, 1, gls_beta)
beta_control = apply(simu_control, 1, gls_beta)
plist = c(plist,t.test(x = beta_case, y = beta_control)$p.value)
}
plist = NULL
for(iter in c(1:1000)){
# print(iter)
simu_case = simulate_traditional(mean_alpha = mean_alpha, sd_alpha = sd_alpha, mean_beta = mean_beta_case, sd_within = sd_within,
sd_beta = sd_beta_case, n = n)
simu_control = simulate_traditional(mean_alpha = mean_alpha, sd_alpha = sd_alpha, mean_beta = mean_beta_control, sd_within = sd_within,
sd_beta = sd_beta_control, n = n)
beta_case = apply(simu_case, 1, gls_beta)
beta_control = apply(simu_control, 1, gls_beta)
plist = c(plist,t.test(x = beta_case, y = beta_control)$p.value)
}
n
plist = NULL
for(iter in c(1:1000)){
print(iter)
simu_case = simulate_traditional(mean_alpha = mean_alpha, sd_alpha = sd_alpha, mean_beta = mean_beta_case, sd_within = sd_within,
sd_beta = sd_beta_case, n = n)
simu_control = simulate_traditional(mean_alpha = mean_alpha, sd_alpha = sd_alpha, mean_beta = mean_beta_control, sd_within = sd_within,
sd_beta = sd_beta_control, n = n)
beta_case = apply(simu_case, 1, gls_beta)
beta_control = apply(simu_control, 1, gls_beta)
plist = c(plist,t.test(x = beta_case, y = beta_control)$p.value)
}
plist
sum(plist < 0.05)/1000
2*1.2
2*1.4
2*1.6
install.packages(c("pkgconfig", "processx", "rlang"))
act  = read.csv("~/Dropbox/Junrui Di/shiny apps/actigraphy-profiles/data/act.csv",stringsAsFactors = F)
wear = read.csv("~/Dropbox/Junrui Di/shiny apps/actigraphy-profiles/data/wear.csv", stringsAsFactors = F)
save(act, file = "~/Desktop/new/data/act.rda")
save(wear, file = "~/Desktop/new/data/wear.rda")
rm(list = ls())
source("Desktop/new/plot_profile.R")
rm(list = ls())
source("Desktop/new/plot_profile.R")
library(shiny)
rm(list = ls())
source("Desktop/new/plot_profile.R")
actpath = "Desktop/new/data/act.rda"
wearpath = "Desktop/new/data/wear.rda"
getDate = function(actpath = actpath, wearpath = NULL){
load(actpath)
if(is.null(wear)){
wear = NULL
}else{
load(wearpath)
}
}
getData(actpath = actpath = )
getData(actpath = actpath )
rm(list = ls())
source("Desktop/new/plot_profile.R")
actpath = "Desktop/new/data/act.rda"
wearpath = "Desktop/new/data/wear.rda"
getData = function(actpath = actpath, wearpath = NULL){
load(actpath)
if(is.null(wear)){
wear = NULL
}else{
load(wearpath)
}
}
library(shiny)
getData(actpath = actpath )
rm(list = ls())
source("Desktop/new/plot_profile.R")
actpath = "Desktop/new/data/act.rda"
wearpath = "Desktop/new/data/wear.rda"
getData = function(actpath = actpath, wearpath = NULL){
load(actpath)
if(is.null(wearpath)){
wear = NULL
}else{
load(wearpath)
}
}
library(shiny)
getData(actpath = actpath )
getData(actpath = actpath)
load(actpath)
rm(list = ls())
source("Desktop/new/plot_profile.R")
actpath = "Desktop/new/data/act.csv"
wearpath = "Desktop/new/data/wear.csv"
getData = function(actpath = actpath, wearpath = NULL){
act = read.csv(actpath, stringsAsFactors = F)
if(is.null(wearpath)){
wear = NULL
}else{
wear = read.csv(wearpath, stringsAsFactors = F)
}
return(act, wear)
}
library(shiny)
act = getData(actpath = actpath)
rm(list = ls())
source("Desktop/new/plot_profile.R")
actpath = "Desktop/new/data/act.csv"
wearpath = "Desktop/new/data/wear.csv"
getData = function(actpath = actpath, wearpath = NULL){
act = read.csv(actpath, stringsAsFactors = F)
if(is.null(wearpath)){
wear = NULL
}else{
wear = read.csv(wearpath, stringsAsFactors = F)
}
return(list(act = act, wear = wear))
}
library(shiny)
act = getData(actpath = actpath)
View(act)
rm(list = ls())
source("Desktop/new/plot_profile.R")
actpath = "Desktop/new/data/act.csv"
wearpath = "Desktop/new/data/wear.csv"
getData = function(actpath = actpath, wearpath = NULL){
act = read.csv(actpath, stringsAsFactors = F)
if(is.null(wearpath)){
wear = NULL
}else{
wear = read.csv(wearpath, stringsAsFactors = F)
}
return(list(act = act, wear = wear))
}
library(shiny)
act = getData(actpath = actpath)$act
wear = getData(actpath = actpath)$wear
runApp('Desktop/new')
runApp('Desktop/new')
rm(list = ls())
source("Desktop/new/plot_profile.R")
actpath = "Desktop/new/data/act.csv"
flagpath = "Desktop/new/data/wear.csv"
getData = function(actpath = actpath, flagpath = NULL){
act = read.csv(actpath, stringsAsFactors = F)
if(is.null(flagpath)){
flag = NULL
}else{
flag = read.csv(flagpath, stringsAsFactors = F)
}
return(list(act = act, flag = flag))
}
library(shiny)
# To run without flag
# act = getData(actpath = actpath)$act
# flag = getData(actpath = actpath)$flag
# To run with wear flag
act = getData(actpath = actpath, flagpath = flagpath)$act
flag = getData(actpath = actpath,flagpath = flagpath)$flag
runApp('Desktop/new')
rm(list = ls())
source("Desktop/new/plot_profile.R")
actpath = "Desktop/new/data/act.csv"
flagpath = "Desktop/new/data/wear.csv"
getData = function(actpath = actpath, flagpath = NULL){
act = read.csv(actpath, stringsAsFactors = F)
if(is.null(flagpath)){
flag = NULL
}else{
flag = read.csv(flagpath, stringsAsFactors = F)
}
return(list(act = act, flag = flag))
}
library(shiny)
# To run without flag
# act = getData(actpath = actpath)$act
# flag = getData(actpath = actpath)$flag
# To run with wear flag
act = getData(actpath = actpath, flagpath = flagpath)$act
flag = getData(actpath = actpath,flagpath = flagpath)$flag
runApp()
getwd()
list.files()
rm(list = ls())
source("plot_profile.R")
getwd()
install.packages(c("accelerometry", "callr", "plotrix", "wordcloud"))
library(wordcloud)
?wordcloud
wordcloud(c(letters, LETTERS, 0:9), seq(1, 1000, len = 62))
wordcloud(c("fragmentattion","accelerometry"))
wordcloud(c("fragmentattion","accelerometry","statistics","data","check"))
install.packages(c("ggpubr", "xtable"))
977-914
63/100000*100
63/100000
install.packages("chron")
library(accelerometry)
library(actigraphy)
12000*1/14
12000*1.14
install.packages(c("digest", "later", "magic", "pkgbuild"))
install.packages("zoo")
library(actigraphy)
devtools::install_github("junruidi/actigraphy")
library(actigraphy)
library(refund)
?pffr
2/50
6/8
2250-3460
3780-3460
3780-3460
320-1210
320/1210
(1-0.668)/0.3319
712/1413''
712/1413
(1-0.668)/0.5
1-(1-0.668)/0.5
1413-712
712*0.688
944-490
701-454
247/701
library(ajive)
?ajive
install.packages(c("cli", "covr", "data.table", "pROC", "robustbase"))
install.packages("robustbase")
devtools::install_github("junruidi/actigraphy")
install.packages(c("betareg", "commonmark", "data.table", "grpreg", "nloptr", "robustbase"))
knitr::opts_chunk$set(echo = TRUE)
frag_bactl = coxph(Surv(time, event) ~ female + factor(race) +  smoking_cat + alcohol_cat +
srhealth + comorbid2 + Age +
employ + education + bmi + scale(lambda.a10) * B_actl  ,data = surv)
?cat
library(actigraphy)
?fragmentation_long
require("rnhanesdata")
data("PAXINTEN_C");data("PAXINTEN_D")
PAXINTEN_C[,paste0("MIN",1:1440)] <- PAXINTEN_C[,paste0("MIN",1:1440)]*Flags_C[,paste0("MIN",1:1440)]
PAXINTEN_D[,paste0("MIN",1:1440)] <- PAXINTEN_D[,paste0("MIN",1:1440)]*Flags_D[,paste0("MIN",1:1440)]
head(names(PAXINTEN_C))
rm(list = ls())
fragmentation_long()
fragmentation_long
fragmentation
library(actigraphy)
fragmentation
fragmentation_long
library(splines)
x = seq(-5,5,100)
?smooth.spline
x <- seq(-5, 5, length = 100)
b <- bs(x, degree = 1, knots = 0)
b
?ns
?bs
?bs
summary(fm1 <- lm(weight ~ bs(height, df = 5), data = women))
fm1
fm1$residuals
women
within(women,plot(height,weight))
install.packages(rms)
install.packages("rms")
require(rms)
f <- ols(y ~ rcs(x, 3))  # 2 d.f. for x
Function(f)  # represent fitted function in simplest R form
latex(f)
x   <- c(0.2,   0.23,   0.26,   0.29,   0.33,   0.46,    0.53 )
y   <- c(0.211, 0.2026, 0.2034, 0.2167, 0.2177, 0.19225, 0.182)
f <- ols(y ~ rcs(x, 3))
latex(f)
Function(f)
summary(f)
f
?rcs
Choi L, NLiu Z, Matthews CE, Buchowski MS. Validation of Accelerometer Wear and
600 Nonwear Time Classification Algorithm.
counts.part1 <- unidata[unidata[, "seqn"] == 21005, "paxinten"]
data(unidata)
library(accelerometry)
data(unidata)
counts.part1 <- unidata[unidata[, "seqn"] == 21005, "paxinten"]
plot(counts.part1)
weartime.flag <- weartime(counts = counts.part1)
par(mfrow = c(1,2))
plot(counts.part1)
plot(weartime.flag)
par(mfrow = c(1,1))
plot(counts.part1)
abline(h = which(weartime.flag == 1),col = "grey")
plot(counts.part1)
abline(v = which(weartime.flag == 1),col = "grey")
plot(counts.part1, col = "white")
points(counts.part1)
plot(counts.part1, col = "white")
abline(v = which(weartime.flag == 1),col = "grey")
points(counts.part1)
?weartime
install.packages(c("data.table", "evaluate", "fansi", "gamlss", "gamlss.data", "gamlss.dist", "mime", "nloptr", "plotrix", "R6", "Rcpp", "rstudioapi"))
library(survminer)
library(survival)
fit <- survfit(Surv(time, status) ~ ph.ecog, data = lung)
ggsurvplot(fit, palette = "jco",
linetype = c("solid", "longdash", "dotted", "dotdash", "dashed"),
legend = "right", legend.title = "Why U no work?!", legend.labs = 1:4)+
guides(color = guide_legend(keywidth = 2, keyheight = 1.8))
devtools::install_github("andrew-leroux/rnhanesdata")
rm(list=ls())
## Please read these comments before running the code contained in this R script!
##
## In order for the code to execute, users need to set the object: dir_supplemental (line 133)
## to be the parent directory of the "code" folder contained in the supplemental material.
## For additional details regarding the analyses performed here, please refer to the manuscript.
##
## The code below is divided into the following "chunks"
##  - Section 0: Install and load all necessary packages, including the rnhanesdata package
##               which contains the accelerometry data used in our analysis.
##  - Section 1:
##              1a. Load and merge accelerometry, demographic/comorbidity, and mortality data
##              1b. Create new factor variables which we will use in our analysis.
##                  * We collapse most comorbidity data into either "Yes", "No", or msising from NHANES
##                    questionairre data which allows for "don't know" or "refused" as responses.
##                     We assume individuals who respond "don't know" or "missing" do not have that particular condition.
##                  * We collapse adult education into 3 levels: "less than high school", "high school", and "more than highschool"
##                  * We add a level "missing" level to alcohol consumption in order to retain individuals with missing data
##                    for this item in our analysis.
##
##  - Section 2: Calculate commonly used accelerometry summary measures:
##                 * TAC: Total activity counts
##                 * TLAC: Total log(1+activity countss)
##                 * WT: Total wear time
##                 * ST: Total sedentary time
##                 * MVPA: Total time spent in MVPA
##                 * SBout: Average duration of sedentary bouts (bouts defined as 1 minute or more)
##                 * ABout: Average duration of active bouts (bouts defined as 1 minute or more)
##                 * SATP: Transition probability from sedentary to active (SATP = 1/SBout)
##                 * ASTP: Transition probability from active to sedentary (ASTP = 1/ABout)
##               Note that in this section these variables are calculated at the day level. After applying exlcusion criteria
##               in Section 3, we average across days within individuals to get one number per measure for each participant.
##               The exclusion criteria involves excluding days which are deemed to have insufficient wear-time (10 hours)
##
##  - Section 3: Apply exclusion criteria and create a data frame with one row per subject which will be used as a basis for regression
##               analyses. This data frame is called "data_analysis".
##
##               When estimating complex survey generalized linear models, we create a svydesign() object via the survey package which
##               uses "data_analysis". In order to fit models which use both  the "adjusted" or "unadjusted" survey weights, we create two separate
##               svydesign() objects. This is done in Section 4.d when we perform forward selection.
##
##               Once we've subset the data to obtain "data_analysis", we calculate adjusted
##               survey weights. These weights are calculated using the reweight_accel() function
##               (see ?reweight_accel for help) which re-weights observed participants using age, gender,
##               and ethnicity strata (among other things). The "adjusted" weights we use for regression analyses are "wtmec4yr_adj_norm"
##
##               Ecxlusion criteria
##                 * Apply age exclusion (i.e. younger than 50, or 85 and over). Note that individuals age 85 and over
##                   at the time of accelerometer wear have NA (missing) values for the variable RIDAGEEX which records
##                   age in months at the time individuals took part in the exam portion of the study.
##                 * Create a table of pairwise missing data on variables that we intend to include in our prediction model to
##                   see the distribution of missing data. In our analysis individuals are only excluded for:
##                      - Missing body mass index (BMI)
##                      - Missing education
##                      - "Bad" accelerometry data
##                              + fewer than 3 days of accelerometry data with at least 10 hours of estimated wear-time
##                              + device calibration flag recorded by NHANES
##                              + data reliability flag recorded by NHANES
##                      - Missing mortality data
##                      - Individuals recorded as "alive" but had fewer than 5 years of follow-up
##
##  - Section 4: Data analysis
##              4a. Perform (unweighted) functional principal component analysis (FPCA) and survey weighted PCA
##              4b. Use backward selection to identify FPCA features associated with 5-year mortality. Find surrogate measures on
##                  the log transformed activity counts that correlate strongly with the features which are associated with 5-year mortality.
##              4c. Perform scalar on function regression (SoFR) where we include individuals' average (log-transformed) activity profiles
##                  as the functional predictor.
##              4d. Use forward selection to evaluate the predictive value of our set of variables identified in 4b/4c as well as the commonly used
##                  accelerometry features calculated in Section 2 of this code.
##
##                  * Note that functional forms are assumed to be linear and no interactions/effect modifications are considered.
##
##
##
##  By default, the assummption is that this R script will be downloaded with the supplemental material which accompanies
##  the manuscript "Organizing and analyzing the activity in NHANES". This supplemental material is a zipped file with 3 folders:
##    - code
##    - figures
##    - tables
##  Assuming users correctly set the variable "dir_supplemental",
##  running this script will save all figure output to the "figures" folder and
##  .tex files which contain the latex version of tables presented in the manuscript.
##
##  Because the data are relatively large and require a non-trivial amount of working memory (RAM)
##  we "clean up" the workspace as we go, clearing items after the relevant figures/tables/regression results are
##  created/printed to the console. This may need to be taken into consideration if there's a particualr area you'd like to investigate in more detail.
##
##  Finally, although most of the code executes fairly quickly, there are two sections that may take a few minutes to run.
##  The first, calculating survey weighted principal components, will not execute by default. This can be changed by switching
##  make_plot_fpca_vs_svypca to TRUE in Section 0.
##  The other chunk that takes time to run is the forward selection procedure. There's a print statement embedded in the code that will
##  report progress on the procedure, but this may take 20-30 minutes to completely finish.
########################################
##                                    ##
##  Section 0: load required packages ##
##                                    ##
########################################
## Check for packages needed to run analyses/install the rnhanesdata package.
## Note: all these packages are available on CRAN, the rnhanesdata package is not due to
##       package size
pckgs <- c("tableone","knitr","kableExtra",   ## packages used for creating Table 1
"devtools",                        ## package used to download R packages stored on GitHub
"MASS",                            ## package used in backward selection
"magrittr","dplyr",                ## packages for merging/transforming data
"survey",                          ## package used for analyzing complex survey data in R
"mgcv","refund"                    ## packages used for smoothing/functional regression
)
sapply(pckgs, function(x) if(!require(x,character.only=TRUE,quietly=TRUE)) {
install.packages(x)
require(x, character.only=TRUE)
})
rm(list=c("pckgs"))
## install the rnhanesdata package and dependencies
## note this may take a few minutes because of the size of the data package
if(!require("rnhanesdata")){
install_github("andrew-leroux/rnhanesdata")
}
require("rnhanesdata")
make_plots  <- TRUE  ## change to FALSE if you don't want to create the figures presented in the manuscript
make_tables <- TRUE  ## change to FALSE if you don't want to create the tables presented in the manuscript
## change "make_plot_fpca_vs_svypca" below to TRUE if you want to plot the first 16 survey weighted principal components versus
## the unweighted functional principal components
make_plot_fpca_vs_svypca <- FALSE
dir_supplemental <- ".."  ## directory where supplemental material folder is saved
code_path        <- file.path(dir_supplemental, "supplemental_material", "code")     ## file path where helper functions and code to create figures are located
figure_path      <- file.path(dir_supplemental, "supplemental_material", "figures")  ## file path where figures will be saved
table_path       <- file.path(dir_supplemental, "supplemental_material", "tables")   ## file path where tables will be saved
source(file.path(code_path,"helper_fns.R"))
#######################################
##                                   ##
##  Section 1a: load and merge data  ##
##                                   ##
#######################################
## load the data
data("PAXINTEN_C");data("PAXINTEN_D")
data("Flags_C");data("Flags_D")
data("Mortality_2011_C");data("Mortality_2011_D")
data("Covariate_C");data("Covariate_D")
## re-code activity counts which are considered "non-wear" to be 0
## this doesn't impact much data, most estimated non-wear times correspond to 0 counts anyway
PAXINTEN_C[,paste0("MIN",1:1440)] <- PAXINTEN_C[,paste0("MIN",1:1440)]*Flags_C[,paste0("MIN",1:1440)]
PAXINTEN_D[,paste0("MIN",1:1440)] <- PAXINTEN_D[,paste0("MIN",1:1440)]*Flags_D[,paste0("MIN",1:1440)]
library(rnhanesdata)
data("PAXINTEN_C")
data("PAXINTEN_D"")
"
data("PAXINTEN_D")
length(unique(PAXINTEN_C$SEQN)) + length(unique(PAXINTEN_D$SEQN))
pt(q =0.05, df = 94, lower.tail = T)
pt(q =0.05, df = 94, lower.tail = F)
pt(q =0.95, df = 94, lower.tail = T)
pt(q =0.95, df = 94, lower.tail = T)-1
pt(q =0.975, df = 94, lower.tail = T)-1
qt(p = 0.025, df = 94,lower.tail = T)
qt(p = 0.025, df = 94,lower.tail = F)
install.packages(c("digest", "dplyr", "MASS", "pkgbuild", "pkgload", "ps", "survey", "testthat", "tidyselect"))
install.packages(c("covr", "devtools", "NLP"))
447/3
rm(list = ls())
ls
list.files
21/(360+358)
360/(360+358) +21/(360+358) - 5/(360+358)
pnorm(1.645, lower.tail = T)
1.96*1.43
setwd("~/")
getwd()
install.packages(c("devtools", "ggplot2", "mgcv", "prettyR", "rlang", "servr", "tidyr", "tinytex", "xfun"))
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("~/Dropbox/web/junruidi.github.io/")
#render your sweet site.
rmarkdown::render_site()
# git add -A
# git commit -m "2nd commit"
