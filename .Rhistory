PAXINTEN_D[,paste0("MIN",1:1440)] <- PAXINTEN_D[,paste0("MIN",1:1440)]*Flags_D[,paste0("MIN",1:1440)]
head(names(PAXINTEN_C))
rm(list = ls())
fragmentation_long()
fragmentation_long
fragmentation
library(actigraphy)
fragmentation
fragmentation_long
library(splines)
x = seq(-5,5,100)
?smooth.spline
x <- seq(-5, 5, length = 100)
b <- bs(x, degree = 1, knots = 0)
b
?ns
?bs
?bs
summary(fm1 <- lm(weight ~ bs(height, df = 5), data = women))
fm1
fm1$residuals
women
within(women,plot(height,weight))
install.packages(rms)
install.packages("rms")
require(rms)
f <- ols(y ~ rcs(x, 3))  # 2 d.f. for x
Function(f)  # represent fitted function in simplest R form
latex(f)
x   <- c(0.2,   0.23,   0.26,   0.29,   0.33,   0.46,    0.53 )
y   <- c(0.211, 0.2026, 0.2034, 0.2167, 0.2177, 0.19225, 0.182)
f <- ols(y ~ rcs(x, 3))
latex(f)
Function(f)
summary(f)
f
?rcs
Choi L, NLiu Z, Matthews CE, Buchowski MS. Validation of Accelerometer Wear and
600 Nonwear Time Classification Algorithm.
counts.part1 <- unidata[unidata[, "seqn"] == 21005, "paxinten"]
data(unidata)
library(accelerometry)
data(unidata)
counts.part1 <- unidata[unidata[, "seqn"] == 21005, "paxinten"]
plot(counts.part1)
weartime.flag <- weartime(counts = counts.part1)
par(mfrow = c(1,2))
plot(counts.part1)
plot(weartime.flag)
par(mfrow = c(1,1))
plot(counts.part1)
abline(h = which(weartime.flag == 1),col = "grey")
plot(counts.part1)
abline(v = which(weartime.flag == 1),col = "grey")
plot(counts.part1, col = "white")
points(counts.part1)
plot(counts.part1, col = "white")
abline(v = which(weartime.flag == 1),col = "grey")
points(counts.part1)
?weartime
install.packages(c("data.table", "evaluate", "fansi", "gamlss", "gamlss.data", "gamlss.dist", "mime", "nloptr", "plotrix", "R6", "Rcpp", "rstudioapi"))
library(survminer)
library(survival)
fit <- survfit(Surv(time, status) ~ ph.ecog, data = lung)
ggsurvplot(fit, palette = "jco",
linetype = c("solid", "longdash", "dotted", "dotdash", "dashed"),
legend = "right", legend.title = "Why U no work?!", legend.labs = 1:4)+
guides(color = guide_legend(keywidth = 2, keyheight = 1.8))
devtools::install_github("andrew-leroux/rnhanesdata")
rm(list=ls())
## Please read these comments before running the code contained in this R script!
##
## In order for the code to execute, users need to set the object: dir_supplemental (line 133)
## to be the parent directory of the "code" folder contained in the supplemental material.
## For additional details regarding the analyses performed here, please refer to the manuscript.
##
## The code below is divided into the following "chunks"
##  - Section 0: Install and load all necessary packages, including the rnhanesdata package
##               which contains the accelerometry data used in our analysis.
##  - Section 1:
##              1a. Load and merge accelerometry, demographic/comorbidity, and mortality data
##              1b. Create new factor variables which we will use in our analysis.
##                  * We collapse most comorbidity data into either "Yes", "No", or msising from NHANES
##                    questionairre data which allows for "don't know" or "refused" as responses.
##                     We assume individuals who respond "don't know" or "missing" do not have that particular condition.
##                  * We collapse adult education into 3 levels: "less than high school", "high school", and "more than highschool"
##                  * We add a level "missing" level to alcohol consumption in order to retain individuals with missing data
##                    for this item in our analysis.
##
##  - Section 2: Calculate commonly used accelerometry summary measures:
##                 * TAC: Total activity counts
##                 * TLAC: Total log(1+activity countss)
##                 * WT: Total wear time
##                 * ST: Total sedentary time
##                 * MVPA: Total time spent in MVPA
##                 * SBout: Average duration of sedentary bouts (bouts defined as 1 minute or more)
##                 * ABout: Average duration of active bouts (bouts defined as 1 minute or more)
##                 * SATP: Transition probability from sedentary to active (SATP = 1/SBout)
##                 * ASTP: Transition probability from active to sedentary (ASTP = 1/ABout)
##               Note that in this section these variables are calculated at the day level. After applying exlcusion criteria
##               in Section 3, we average across days within individuals to get one number per measure for each participant.
##               The exclusion criteria involves excluding days which are deemed to have insufficient wear-time (10 hours)
##
##  - Section 3: Apply exclusion criteria and create a data frame with one row per subject which will be used as a basis for regression
##               analyses. This data frame is called "data_analysis".
##
##               When estimating complex survey generalized linear models, we create a svydesign() object via the survey package which
##               uses "data_analysis". In order to fit models which use both  the "adjusted" or "unadjusted" survey weights, we create two separate
##               svydesign() objects. This is done in Section 4.d when we perform forward selection.
##
##               Once we've subset the data to obtain "data_analysis", we calculate adjusted
##               survey weights. These weights are calculated using the reweight_accel() function
##               (see ?reweight_accel for help) which re-weights observed participants using age, gender,
##               and ethnicity strata (among other things). The "adjusted" weights we use for regression analyses are "wtmec4yr_adj_norm"
##
##               Ecxlusion criteria
##                 * Apply age exclusion (i.e. younger than 50, or 85 and over). Note that individuals age 85 and over
##                   at the time of accelerometer wear have NA (missing) values for the variable RIDAGEEX which records
##                   age in months at the time individuals took part in the exam portion of the study.
##                 * Create a table of pairwise missing data on variables that we intend to include in our prediction model to
##                   see the distribution of missing data. In our analysis individuals are only excluded for:
##                      - Missing body mass index (BMI)
##                      - Missing education
##                      - "Bad" accelerometry data
##                              + fewer than 3 days of accelerometry data with at least 10 hours of estimated wear-time
##                              + device calibration flag recorded by NHANES
##                              + data reliability flag recorded by NHANES
##                      - Missing mortality data
##                      - Individuals recorded as "alive" but had fewer than 5 years of follow-up
##
##  - Section 4: Data analysis
##              4a. Perform (unweighted) functional principal component analysis (FPCA) and survey weighted PCA
##              4b. Use backward selection to identify FPCA features associated with 5-year mortality. Find surrogate measures on
##                  the log transformed activity counts that correlate strongly with the features which are associated with 5-year mortality.
##              4c. Perform scalar on function regression (SoFR) where we include individuals' average (log-transformed) activity profiles
##                  as the functional predictor.
##              4d. Use forward selection to evaluate the predictive value of our set of variables identified in 4b/4c as well as the commonly used
##                  accelerometry features calculated in Section 2 of this code.
##
##                  * Note that functional forms are assumed to be linear and no interactions/effect modifications are considered.
##
##
##
##  By default, the assummption is that this R script will be downloaded with the supplemental material which accompanies
##  the manuscript "Organizing and analyzing the activity in NHANES". This supplemental material is a zipped file with 3 folders:
##    - code
##    - figures
##    - tables
##  Assuming users correctly set the variable "dir_supplemental",
##  running this script will save all figure output to the "figures" folder and
##  .tex files which contain the latex version of tables presented in the manuscript.
##
##  Because the data are relatively large and require a non-trivial amount of working memory (RAM)
##  we "clean up" the workspace as we go, clearing items after the relevant figures/tables/regression results are
##  created/printed to the console. This may need to be taken into consideration if there's a particualr area you'd like to investigate in more detail.
##
##  Finally, although most of the code executes fairly quickly, there are two sections that may take a few minutes to run.
##  The first, calculating survey weighted principal components, will not execute by default. This can be changed by switching
##  make_plot_fpca_vs_svypca to TRUE in Section 0.
##  The other chunk that takes time to run is the forward selection procedure. There's a print statement embedded in the code that will
##  report progress on the procedure, but this may take 20-30 minutes to completely finish.
########################################
##                                    ##
##  Section 0: load required packages ##
##                                    ##
########################################
## Check for packages needed to run analyses/install the rnhanesdata package.
## Note: all these packages are available on CRAN, the rnhanesdata package is not due to
##       package size
pckgs <- c("tableone","knitr","kableExtra",   ## packages used for creating Table 1
"devtools",                        ## package used to download R packages stored on GitHub
"MASS",                            ## package used in backward selection
"magrittr","dplyr",                ## packages for merging/transforming data
"survey",                          ## package used for analyzing complex survey data in R
"mgcv","refund"                    ## packages used for smoothing/functional regression
)
sapply(pckgs, function(x) if(!require(x,character.only=TRUE,quietly=TRUE)) {
install.packages(x)
require(x, character.only=TRUE)
})
rm(list=c("pckgs"))
## install the rnhanesdata package and dependencies
## note this may take a few minutes because of the size of the data package
if(!require("rnhanesdata")){
install_github("andrew-leroux/rnhanesdata")
}
require("rnhanesdata")
make_plots  <- TRUE  ## change to FALSE if you don't want to create the figures presented in the manuscript
make_tables <- TRUE  ## change to FALSE if you don't want to create the tables presented in the manuscript
## change "make_plot_fpca_vs_svypca" below to TRUE if you want to plot the first 16 survey weighted principal components versus
## the unweighted functional principal components
make_plot_fpca_vs_svypca <- FALSE
dir_supplemental <- ".."  ## directory where supplemental material folder is saved
code_path        <- file.path(dir_supplemental, "supplemental_material", "code")     ## file path where helper functions and code to create figures are located
figure_path      <- file.path(dir_supplemental, "supplemental_material", "figures")  ## file path where figures will be saved
table_path       <- file.path(dir_supplemental, "supplemental_material", "tables")   ## file path where tables will be saved
source(file.path(code_path,"helper_fns.R"))
#######################################
##                                   ##
##  Section 1a: load and merge data  ##
##                                   ##
#######################################
## load the data
data("PAXINTEN_C");data("PAXINTEN_D")
data("Flags_C");data("Flags_D")
data("Mortality_2011_C");data("Mortality_2011_D")
data("Covariate_C");data("Covariate_D")
## re-code activity counts which are considered "non-wear" to be 0
## this doesn't impact much data, most estimated non-wear times correspond to 0 counts anyway
PAXINTEN_C[,paste0("MIN",1:1440)] <- PAXINTEN_C[,paste0("MIN",1:1440)]*Flags_C[,paste0("MIN",1:1440)]
PAXINTEN_D[,paste0("MIN",1:1440)] <- PAXINTEN_D[,paste0("MIN",1:1440)]*Flags_D[,paste0("MIN",1:1440)]
library(rnhanesdata)
data("PAXINTEN_C")
data("PAXINTEN_D"")
"
data("PAXINTEN_D")
length(unique(PAXINTEN_C$SEQN)) + length(unique(PAXINTEN_D$SEQN))
pt(q =0.05, df = 94, lower.tail = T)
pt(q =0.05, df = 94, lower.tail = F)
pt(q =0.95, df = 94, lower.tail = T)
pt(q =0.95, df = 94, lower.tail = T)-1
pt(q =0.975, df = 94, lower.tail = T)-1
qt(p = 0.025, df = 94,lower.tail = T)
qt(p = 0.025, df = 94,lower.tail = F)
install.packages(c("digest", "dplyr", "MASS", "pkgbuild", "pkgload", "ps", "survey", "testthat", "tidyselect"))
install.packages(c("covr", "devtools", "NLP"))
447/3
rm(list = ls())
ls
list.files
21/(360+358)
360/(360+358) +21/(360+358) - 5/(360+358)
pnorm(1.645, lower.tail = T)
1.96*1.43
setwd("~/")
getwd()
install.packages(c("devtools", "ggplot2", "mgcv", "prettyR", "rlang", "servr", "tidyr", "tinytex", "xfun"))
install.packages(c("pkgload", "survival"))
install.packages(c("lattice", "MASS", "Matrix", "ModelMetrics", "psych", "remotes", "shiny"))
library(refund)
View(mfpca.sc())
View(mfpca.sc
)
?quadWeights
View(mfpca.sc)
?mfpca.sc
install.packages(c("epiR", "ipred", "ps", "rcmdcheck", "sessioninfo"))
data(DTI)
DTI = subset(DTI, Nscans < 6)  ## example where all subjects have 6 or fewer visits
id  = DTI$ID
Y = DTI$cca
library(refund)
data(DTI)
DTI = subset(DTI, Nscans < 6)  ## example where all subjects have 6 or fewer visits
id  = DTI$ID
Y = DTI$cca
id
head(id)
table(id)
?matrix
D = 3
J = 4
ueta = matrix(0, D, J)
eta = matrix(0, D, J)
eta
J = 10
Y.split = as.list(rep(NA, J))
Y.split
N = 10
d.vec.split = rep(1:N, each = 10)
d.vec.split = rep(1:N, each = 12)
d.vec.split
argvals = seq(0, 1, , 10)
argvals
Y = matrix(rnorm(100),ncol = 20)
Y
Y[0:4]
Y[0:4,]
############################################################################
###
###     This file contains R codes to conduct simulation studies to
###     evaluate the performance of the MFPCA.
###
###     The true data are generated from Case 2 in the paper, and the
###     function "MFPCA" are called to fit the MFPCA model. The level 1
###     eigenfunctions are four sine and cosine functions, and the level 2
###     eigenfunctions are muturally orthogonal polynomials.
###
###
###         Author: Chongzhi Di
###         Based on the work with Ciprian M.  Crainiceanu
###
###         The corresponding paper is under review
###         Date:       06/20/2007
##########################################################################
############## simulation of multilevel functional data
###     Set the seed for simulation
set.seed(1371)
###     K1: number of true principal components
###     K2: number of true principal components
###     K = K1 + K2 : number of total principal components
###     The true data is generated with K1=4 and K2=4
###     N: number of grid points for each function
###     M: number of clusters of functions
###     J: number of functions per cluster
###     t: actual knots for each function
###     lambda: true eigen values
###     f:      true eigen functions
K1 <- 4
K2 <- 4
K <- K1 + K2
N <- 100
###     generate the true eigenvalues
lambda1 <-  0.5^(0:(K1-1))
lambda2 <-  0.5^(0:(K2-1))
###     generate the set of equally spaced grid points on which
###     the functions are measured
tlength <- 1
t <- seq(0,tlength,length=N)
###     generate bases of the functional space
###     K base functions
###     The true level 1 eigenfunctions are four muturally orthogonal
###     sine and cosine functions
f1 <- matrix(0,nrow=K1,ncol=N)
for ( i in 1:(K1/2) ) {
f1[2*i-1,] <- sqrt(2/tlength)*sin(i*t*2*pi/tlength)
f1[2*i,] <- sqrt(2/tlength)*cos(i*t*2*pi*tlength)
}
###     The true level 2 eigenfunctions are four muturally orthogonal
###     polynomial functions
tt <- t/tlength
f2 <- matrix(0, nrow=K2, ncol=N)
f2[1,] <- rep(1, N)*sqrt(1/tlength)
f2[2,] <- sqrt(3/tlength) * (2*tt - 1)
f2[3,] <- sqrt(5/tlength) * (6*tt^2 - 6 * tt + 1)
f2[4,] <- sqrt(7/tlength) * (20*tt^3 - 30*tt^2 + 12 * tt -1)
f <- rbind(f1, f2)
###     Calculate the paiwise inner products between eigenfunctions
###     By construction, the norm of each function is 1, i.e, the inner product
###     between each eigenfunction and itself is 1.
###
###     The inner product between any two functions is the cosine of the angle
###     between them. The farther away the inner product is from 0, the more they
###     are departed from orthogonal.
norm <- matrix(0, K, K)
for(i in 1:K)
for(j in 1:K)
norm[i,j] <- sum( f[i,] * f[j,] )*tlength /N
###     Generate M random coefficients si's from normal distributions
###     with variance equal to the corresponding eigenvalue.
M <- 200
J <- 3
si1 <- matrix(0, nrow=M, ncol=K1)
si2 <- matrix(0, nrow=M*J, ncol=K2)
for(k in 1:K1) {
si1[,k] <- rnorm( M,sd=sqrt(lambda1[k]) )
}
for(k in 1:K2) {
si2[,k] <- rnorm( M * J ,sd=sqrt(lambda2[k]) )
}
###     Calculate the random functions as the sum of the products of generated scores
###     and corresponding eigenfunctions.
y <- matrix(0,nrow=M*J,ncol=N)
for(m in 1:M) {
temp <- apply( ( si1[m,] %*% t(rep(1,N)) ) * f1, 2,sum)
for(j in 1:J) {
y[ (m-1)*J + j ,] <- temp + apply( ( si2[(m-1)*J + j ,] %*% t(rep(1,N)) ) * f2, 2,sum)
}
}
View(y)
rm(list = ls())
?mfpca.sc
data(DTI)
DTI = subset(DTI, Nscans < 6)  ## example where all subjects have 6 or fewer visits
id  = DTI$ID
Y = DTI$cca
visit = ave(id, id, FUN = seq_along)
visit
visit = ave(id, id, FUN = seq_along)
rm(list = ls())
data(DTI)
DTI = subset(DTI, Nscans < 6)  ## example where all subjects have 6 or fewer visits
id  = DTI$ID
Y = DTI$cca
visit = ave(id, id, FUN = seq_along)
Y.df = data.frame(id = id, visit = visit)
Y.df$Y = Y.df
View(Y.df)
stopifnot((!is.null(Y) && !is.null(id)))
if (!is.null(visit)) {
visit = as.integer(factor(visit))
}
else {
visit = ave(id, id, FUN = seq_along)
}
rm(list = ls())
data(DTI)
DTI = subset(DTI, Nscans < 6)  ## example where all subjects have 6 or fewer visits
id  = DTI$ID
Y = DTI$cca
visit = ave(id, id, FUN = seq_along)
Y.df = data.frame(id = id, visit = visit)
Y.df$Y = Y
View(Y.df)
stopifnot((!is.null(Y) && !is.null(id)))
if (!is.null(visit)) {
visit = as.integer(factor(visit))
}
else {
visit = ave(id, id, FUN = seq_along)
}
rm(list = ls())
data(DTI)
DTI = subset(DTI, Nscans < 6)  ## example where all subjects have 6 or fewer visits
id  = DTI$ID
Y = DTI$cca
visit = ave(id, id, FUN = seq_along)
Y.df = data.frame(id = id, visit = visit)
Y.df$Y = Y
J = length(unique(visit))
M = length(unique(id))
N = NCOL(Y)
I = NROW(Y)
nVisits = data.frame(table(id))
colnames(nVisits) = c("id", "numVisits")
View(nVisits)
mu <- apply(y, 2,mean)
eta <- matrix(0, J, N)
mu <- apply(Y, 2,mean)
mueta = matrix(0, D, J)
eta = matrix(0, D, J)
Y.split = as.list(rep(NA, J))
Y.split
J
j = 1
Y.split[[j]] = subset(Y.df, visit == j)
subset(Y.df, visit == j)
a = subset(Y.df, visit == j)
View(a)
d.vec.split = rep(1:N, each = NROW(Y.split[[j]]))
d.vec.split
mueta[, j] = apply(Y.split[[j]], 2, mean)
mueta = matrix(0, J, N)
mueta[, j] = apply(Y.split[[j]], 2, mean)
mueta[, j] = apply(Y.split[[j]]$Y, 2, mean)
Y.split[[j]]$Y
a = Y.split[[j]]$Y
View(a)
mueta[j, ] = apply(Y.split[[j]]$Y, 2, mean)
View(mueta)
mueta[j, ] = apply(Y.split[[j]]$Y, 2, mean)
eta[j,] <- mueta[j,] - mu
View(eta)
NROW(Y.split[[j]])
for (j in 1:J) {
Y.split[[j]] = subset(Y.df, visit == j)
mueta[j, ] = apply(Y.split[[j]]$Y, 2, mean)
eta[j,] <- mueta[j,] - mu
Y.split[[j]]$Y.tilde = Y.split[[j]]$Y - matrix(mueta[j,], NROW(Y.split[[j]]), D, byrow = TRUE)
}
NROW(Y.split[[j]])
Y.split = as.list(rep(NA, J))
j = 1
Y.split[[j]] = subset(Y.df, visit == j)
mueta[j, ] = apply(Y.split[[j]]$Y, 2, mean)
eta[j,] <- mueta[j,] - mu
NROW(Y.split[[j]])
Y.split[[j]]$Y.tilde = Y.split[[j]]$Y - matrix(mueta[j,], NROW(Y.split[[j]]), N, byrow = TRUE)
View(Y.split)
for (j in 1:J) {
Y.split[[j]] = subset(Y.df, visit == j)
mueta[j, ] = apply(Y.split[[j]]$Y, 2, mean)
eta[j,] <- mueta[j,] - mu
Y.split[[j]]$Y.tilde = Y.split[[j]]$Y - matrix(mueta[j,], NROW(Y.split[[j]]), N, byrow = TRUE)
}
Y.d
for (j in 1:J) {
Y.split[[j]] = subset(Y.df, visit == j)
mueta[j, ] = apply(Y.split[[j]]$Y, 2, mean)
eta[j,] <- mueta[j,] - mu
Y.split[[j]]$Y.tilde = Y.split[[j]]$Y - matrix(mueta[j,], NROW(Y.split[[j]]), N, byrow = TRUE)
}
View(Y.split)
Y.df.new = Reduce(function(...) merge(..., by = c("id",
"visit", "Y", "Y.tilde"), all = TRUE, sort = FALSE),
Y.split)
Y.df.new = Y.df.new[order(Y.df.new$id, Y.df.new$visit),
]
Y.tilde = Y.df.new$Y.tilde
matrix(mu, I, N, byrow = TRUE)
a = matrix(mu, I, N, byrow = TRUE)
View(a)
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("~/Dropbox/web/junruidi.github.io/")
#render your sweet site.
rmarkdown::render_site()
# git add -A
# git commit -m "2nd commit"
# git push origin master
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("~/Dropbox/web/junruidi.github.io/")
#render your sweet site.
rmarkdown::render_site()
# git add -A
# git commit -m "2nd commit"
# git push origin master
