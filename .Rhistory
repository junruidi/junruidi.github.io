install.packages("Rcpp")
x = c(12,1,3,"a")
x
library(reticulate)
py_available()
install.packages("DT")
install.packages(c("callr", "fs", "gamm4", "glue", "reticulate", "RLRsim", "survey", "tibble", "xml2"))
install.packages("backports")
install.packages("purrr")
install.packages(c("qdap", "xlsx"))
install.packages("rcade")
devtools::install_github('RLesur/Rcade')
Rcade::games$Mariohtml5
Rcade::games$`2048`
Rcade::games$Pond
Rcade::games$CathTheEgg
install.packages(c("fda", "rex", "tibble", "withr"))
install.packages("xml2")
sessionInfo()
library(refund)
sessionInfo()
install.packages(c("labelled", "pkgbuild"))
install.packages(c("git2r", "labelled", "rematch2", "rlang", "usethis", "zoo"))
install.packages(c("ellipsis", "gender", "maptools", "vctrs"))
Rcade::games
Rcade::games$BoulderDash
Rcade::games$SURVIVOR
Rcade::games$Pacman
32+11+8+8+5+4+3+2+2+3
install.packages(c("car", "carData", "sp", "tidyr", "tinytex", "xfun"))
library(refund)
?gls_cs
gls_cs()
gls_cs
install.packages(c("dplyr", "ggplot2", "haven", "later", "pkgload", "rmarkdown", "vctrs"))
library(lubridate)
ymd("2012/1/31")
ymd("2012\1\31")
ymd("2012-1-31")
ymd("2012 1 31")
library(ActFrag)
?fragmentation_long
View(example_activity_data$count)
library(rnhanesdata)
install.packages(c("backports", "tibble"))
?lm
install.packages("backports")
install.packages(c("chron", "cpp11", "dplyr", "Hmisc", "ps", "zip"))
library(rnhanesdata)
devtools::install_github("andrew-leroux/rnhanesdata")
library(rnhanesdata)
library(dplyr)
library(accelerometry)
library(ActFrag)
rm(list = ls())
data("PAXINTEN_C")
data("PAXINTEN_D")
data("Flags_C")
data("Flags_D")
## re-code activity counts which are considered "non-wear" to be 0
## this doesn't impact much data, most estimated non-wear times correspond to 0 counts anyway
PAXINTEN_C[,paste0("MIN",1:1440)] <- PAXINTEN_C[,paste0("MIN",1:1440)]*Flags_C[,paste0("MIN",1:1440)]
PAXINTEN_D[,paste0("MIN",1:1440)] <- PAXINTEN_D[,paste0("MIN",1:1440)]*Flags_D[,paste0("MIN",1:1440)]
act = rbind(PAXINTEN_C,PAXINTEN_D)
flag = rbind(Flags_C, Flags_D)
num_to_week = function(x){
factor(ifelse(x == 1, "Sun", ifelse(x == 2, "Mon", ifelse(x == 3, "Tue",
ifelse(x == 4, "Wed", ifelse(x == 5, "Thu",
ifelse(x == 6, "Fri", ifelse(x == 7, "Sat",NA))))))),
levels = c("Sun","Mon","Tue","Wed","Thu","Fri","Sat"))
}
act$WEEKDAY = num_to_week(act$WEEKDAY)
flag$WEEKDAY = num_to_week(flag$WEEKDAY)
act$Seq = rep(x = c(1:7),length(unique(act$SEQN)))
flag$Seq = rep(x = c(1:7),length(unique(flag$SEQN)))
act$SDDSRVYR = NULL
flag$SDDSRVYR = NULL
act = act[,c(1,2,3,4,1445,5:1444)]
flag = flag[,c(1,2,3,4,1445,5:1444)]
# subject days with more than 10 hours of wearing
nonwear_flag10 = rowSums(flag[, paste('MIN', 1:1440, sep='')], na.rm = TRUE) <600
act10 = act[!(nonwear_flag10 | act$PAXCAL==2 | act$PAXCAL==9 | act$PAXSTAT==2),]
wear10 = flag[!(nonwear_flag10 | flag$PAXCAL==2 | flag$PAXCAL==9 | flag$PAXSTAT==2),]
act10$PAXCAL = act10$PAXSTAT = wear10$PAXCAL = wear10$PAXSTAT = NULL
########################################################################
## This is to correct the period where there is count to nonwear NA/0 ##
########################################################################
act10 = na.omit(act10)
wear10 = na.omit(wear10)
rm(list = setdiff(ls(),c("act10","wear10")))
## Fragmentation
act10$wave = wear10$wave = act10$WEEKDAY = wear10$WEEKDAY =  NULL
names(act10)[c(1:2)] = names(wear10)[c(1:2)] = c("ID","Day")
View(act10)
tail(names(act10))
act_mat = as.matrix(act10[,-c(1:2)])
flag_mat = as.matrix(wear10[,-c(1:2)])
flag_matw = flag_mat
flag_matw[flag_matw == 0] = NA
act_matw = act_mat * flag_matw
metrics = tibble(ID = act10$ID,Day = act10$Day,
LiPA = rowSums(act_mat < 2020 & act_mat >= 100),
MVPA = rowSums(act_mat >= 2020),
WT = rowSums(flag_mat),
LTAC = log(rowSums(act_mat) + 1),
TAC = rowSums(act_mat),
TLAC = rowSums(log(1+act_mat)),
ST = rowSums(act_matw < 100,na.rm = T))
metrics_ave = metrics %>% select(-Day) %>% group_by(ID) %>%
summarise(LiPA = mean(LiPA),
MVPA = mean(MVPA),
WT = mean(WT),
LTAC = mean(LTAC),
TAC = mean(TAC),
TLAC = mean(TLAC),
ST = mean(ST))
frag_10 = fragmentation_long(count.data = act10, weartime = wear10, thresh = 100, bout.length = 1, metrics = "TP", by = "subject")
save(metrics, metrics_ave, file = "C:/Users/STEP/Desktop/PAmetrics.rda")
install.packages(c("backports", "callr", "glue", "jsonlite", "labelled", "maptools", "processx", "refund", "tidyr", "vctrs", "zip"))
install.packages(c("cowplot", "lmtest", "stringi", "xfun", "xlsx"))
install.packages("xlsx")
sessionInfo()
install.packages(c("backports", "covr", "htmlTable", "xlsx"))
install.packages(c("devtools", "knitr", "labelled", "matrixStats", "openssl", "openxlsx", "tinytex", "usethis", "vcd", "withr"))
install.packages("qdap")
install.packages(c("coda", "cpp11", "qdap", "rmarkdown", "xfun"))
install.packages(c("callr", "cli", "clipr", "cpp11", "data.table", "digest", "DT", "e1071", "Formula", "htmlwidgets", "igraph", "labeling", "lme4", "lmerTest", "NLP", "ps", "qdap", "readr", "rlang", "rmarkdown", "sp", "statmod", "stringdist", "tibble"))
install.packages(c("openxlsx", "R6", "xfun"))
install.packages("keras")
library(keras)
library(keras)
?install_keras
install_keras()
y
install_keras(method = "conda",conda = "C:\Users\STEP\anaconda3\python.exe")
install_keras(method = "conda",conda = "C:/Users/STEP/anaconda3/python.exe")
library(reticulate)
py_available()
py_versions_windows()
install_keras(method = "conda",conda = "C:/Users/STEP/anaconda3/python.exe")
py_config()
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
install_keras()
library(keras)
install_keras(method = "conda",conda = "C:/Users/STEP/anaconda3/python.exe")
install_keras()
library(reticulate)
py_config()
library(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
install_tensorflow()
install_keras
install_keras()
library(keras)
minist = dataset_mnist()
use_condaenv("tf")
library(reticulate)
install_keras(method = c("conda"), conda = "auto", version = "default")
library(keras)
install_keras(method = c("conda"), conda = "auto", version = "default")
library(keras)
mnist <- dataset_mnist()
install.packages("tensorflow")
install.packages("tensorflow")
library(tensorflow)
install_tensorflow()
library(keras)
mnist = dataset_mnist()
reticulate::py_config()
tensorflow::tf_config()
install.packages("keras")
library(keras)
install_keras()
reticulate::py_config()
install_keras()
library(reticulate)
use_python("C:/Users/STEP/anaconda3/python.exe")
py_config()
install_keras()
library(keras)
install_keras()
mnist <- dataset_mnist()
library(keras)
use_condaenv("tf")
library(reticulate)
install_keras(method = c("conda"), conda = "auto", version = "default", tensorflow = "gpu")
reticulate::use_condaenv("keras-gpu")
library(keras)
x = dataset_mnist()
install_github("rstudio/keras")
library(devtools)
install_github("rstudio/keras")
library(keras)
install_keras(tensorflow = "1.13.1")
library(keras)
x = dataset_mnist()
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
x_train
dim(x_train) <- c(nrow(x_train), 784)
dim(x_test) <- c(nrow(x_test), 784)
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = "relu", input_shape = c(784)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = "softmax")
summary(model)
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(),
metrics = c("accuracy")
)
history <- model %>% fit(
x_train, y_train,
epochs = 30, batch_size = 128,
validation_split = 0.2
)
plot(history)
model %>% evaluate(x_test, y_test,verbose = 0)
model %>% predict_classes(x_test)
reticulate::py_config()
reticulate::py_config()
library(reticulate)
use_python("C:/Users/STEP/anaconda3/python.exe")
py_config()
py_discover_config()
conda_list()
myenvs = conda_list()
envname = myenvs$name[2]
envname
use_condaenv(envname,required = T)
library(reticulate)
myenvs=conda_list()
envname=myenvs$name[2]
use_condaenv(envname, required = TRUE)
py_config()
library(reticulate)
py_config()
repl_python()
import pandas
import parso
exit
py_config()
librar
library(reticulate)
py_config()
n
py_config()
repl_python()
import pandas
exit
myenvs=conda_list()
envname=myenvs$name[2]
use_condaenv(envname, required = TRUE)
myenvs
library(reticulate)
myenvs=conda_list()
myenvs
use_python("C:/Users/STEP/anaconda3/python.exe")
myenvs=conda_list()
myenvs
library(keras)
x = dataset_imdb()
x = x$train
x = x$x
x
install.packages(c("generics", "testthat"))
library(keras)
## 1.1 Load in data
imdb = dataset_imdb(num_words = 10000)
str(imdb)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb
## 1.2 Encoding the integer sequences into a binary matrix
vectorize_sequences = function(sequences, dimension = 10000) {
results = matrix(0, nrow = length(sequences), ncol = dimension)
for (i in 1:length(sequences)){
results[i, sequences[[i]]] = 1
}
results
}
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)
y_train = as.numeric(train_labels)
y_test = as.numeric(test_labels)
head(x_train)
setwd("C:/Users/STEP/Dropbox/web/junruidi.github.io/")
#render your sweet site.
rmarkdown::render_site()
setwd("C:/Users/STEP/Dropbox/web/junruidi.github.io/")
#render your sweet site.
rmarkdown::render_site()
library(reticulate)
use_python("C:/Users/STEP/anaconda3/python.exe")
py_config()
setwd("C:/Users/STEP/Dropbox/web/junruidi.github.io/")
#render your sweet site.
rmarkdown::render_site()
# git add -A
# git commit -m "2nd commit"
# git push origin master
