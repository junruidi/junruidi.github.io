---
title: "Notes on Optimization"
css: style2.css
---
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-99094202-1', 'auto');
  ga('send', 'pageview');

</script>

#

#

I am currently learning optimization. Here are some of my thoughts and notes.

<center> <h3>Tools</h3> </center>
 
- [NEOS](https://neos-guide.org/): network enabled optimization system.

- [CVXR](https://cran.r-project.org/web/packages/CVXR/index.html): disciplined convex optimization.

<center> <h3>Miscellaneous</h3> </center>

#### 1. Equations systems

If $f(x) = 0$, where $x \in \mathbb{R}^p$, $f \in \mathbb{R}^k$, then

- $k=p$: determined properly

- $k>p$: over determined

- $k<p$: underdetermined (over parametrized)

#### 2. Different convergence rate [More](https://en.wikipedia.org/wiki/Rate_of_convergence)

Consider the series $\{x_k\} \rightarrow x^{*}$

- q linear convergence
$$\text{lim}_{k \rightarrow \infty} \frac{|x_{k+1} - x^*|}{|x_k - x^*|} = c$$
- q super linear convergence
$$\text{lim}_{k \rightarrow \infty} \frac{|x_{k+1} - x^*|}{|x_k - x^*|} \rightarrow 0$$
- q quadratic convergence
$$\text{lim}_{k \rightarrow \infty} \frac{|x_{k+1} - x^*|}{|x_k - x^*|^q} \rightarrow c$$
- q sublinear convergence
$$\text{lim}_{k \rightarrow \infty} \frac{|x_{k+1} - x^*|}{|x_k - x^*|} \rightarrow 1$$

<center> <h3>Numeric Derivative</h3> </center>

- Standard approach (foward/backward)

- Centered approach

- [More readings](http://www2.math.umd.edu/~dlevy/classes/amsc466/lecture-notes/differentiation-chap.pdf) from Doron Levy.

<center> <h3>Newton's Method</h3> </center>
