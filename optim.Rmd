---
title: "Notes on Optimization"
css: style2.css
---
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-99094202-1', 'auto');
  ga('send', 'pageview');

</script>

#

#

I am currently learning optimization (thanks to Dr. Ravi Varadhan and Dr. Vadim Zipunnikov). Here are some of my thoughts and notes.

<center> <h3>Tools</h3> </center>
 
- [NEOS](https://neos-guide.org/): network enabled optimization system.

- [CVXR](https://cran.r-project.org/web/packages/CVXR/index.html): disciplined convex optimization.

---

<center> <h3>Miscellaneous</h3> </center>

#### 1. Equations systems

If $f(x) = 0$, where $x \in \mathbb{R}^p$, $f \in \mathbb{R}^k$, then

- $k=p$: determined properly

- $k>p$: over determined

- $k<p$: underdetermined (over parametrized)

#### 2. Different convergence rate [More](https://en.wikipedia.org/wiki/Rate_of_convergence)

Consider the series $\{x_k\} \rightarrow x^{*}$

- q linear convergence
$$\text{lim}_{k \rightarrow \infty} \frac{|x_{k+1} - x^*|}{|x_k - x^*|} = c$$
- q super linear convergence
$$\text{lim}_{k \rightarrow \infty} \frac{|x_{k+1} - x^*|}{|x_k - x^*|} \rightarrow 0$$
- q quadratic convergence
$$\text{lim}_{k \rightarrow \infty} \frac{|x_{k+1} - x^*|}{|x_k - x^*|^q} \rightarrow c$$
- q sublinear convergence
$$\text{lim}_{k \rightarrow \infty} \frac{|x_{k+1} - x^*|}{|x_k - x^*|} \rightarrow 1$$

#### 3. Gradient, hessian, and Jacobian

For a scalar function $f: \mathbb{R}^p \rightarrow \mathbb{R}$:

The gradient (first order derivatives) is the vector  $\nabla f = (\frac{\partial f}{\partial x_1}, \cdots,\frac{\partial f}{\partial x_p}) $. 

The hessian (second order derivatives) is the matrix $\nabla^2 f = [\frac{\partial^2 f}{\partial x_i \partial x_j}]$.

For a equation systems $F: \mathbb{R}^p \rightarrow \mathbb{R}^p$:

The Jacobian is the matrix $J = [\frac{\partial F_i}{\partial x_j}]$.

---

<center> <h3>Numeric Derivative</h3> </center>

#### 1.  Standard approach (foward/backward)
$$f'(x) \approx \frac{f(x+h) - f(x)}{h} \approx \frac{f(x) - f(x-h)}{h}$$

#### 2. Centered differecing

$$f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}$$
Derivation based on Talor expansion of $f(x+h)$ and $f(x-h)$.

$$f'(x) = \frac{f(x+h) - f(x-h)}{2h} -\frac{h^2}{12}[f'''(\xi_1) + f'''(\xi_2)]$$

#### 3. Selection of $h$
$$h = \sqrt{\epsilon} \max (|x|,\text{typical } x)$$
where $\epsilon$ is the machine constant.

#### 4. Complex step derivation

$$f(x+ih) \approx f(x) + ihf'(x) $$
$$Img(f(x+ih)) \approx hf'(x)$$
$$f'(x) \approx \frac{1}{h}Img(f(x+ih))$$



#### 5. [More readings](http://www2.math.umd.edu/~dlevy/classes/amsc466/lecture-notes/differentiation-chap.pdf) from Doron Levy.

---

<center> <h3>Newton's Method</h3> </center>

#### 1. Univariate scalar function

$f: \mathbb{R}\rightarrow \mathbb{R}$

$$f(x) = f(x_c) + \int_{x_c}^{x}f'(u)du \approx f(x_c) + f'(x_c)(x-xc)$$

The Newton's estimation is 

$$x_{k+1} = x_{k} - \frac{f(x_k)}{f'(x_k)}$$

#### 2. Equation systems

$F: \mathbb{R}^p\rightarrow \mathbb{R}^p$, and the Jacobian $J = \frac{\partial F_i(x)}{\partial x_j}$

$$F(x_c + p) = F(x_c) + \int_{x_c}^{x}J(t)dt \approx F(x_c) + J(x_c)p$$
The Newton's estimation is 

$$x_{k+1} = x_{k} - F(x_c)J^{-1}(x_c)$$

#### 3. Quasi Newton

The goal is to not calculate $J$ for all the iterations, instead using the Broydan's rank 1 update $x_{k+1} = x_{k} - F(x_c)B_c^{-1}$.

$$B_{k+1} = B_k + \frac{(y - B_k s)s^T}{s^Ts}$$
where $y = F(x_{k+1}) - F(x_k)$, and step $s = x_{k+1} - x_k$. Eventually, the inverse can be done using the sherman morrison formula.

