---
title: "Notes on Optimization"
css: style2.css
---
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-99094202-1', 'auto');
  ga('send', 'pageview');

</script>

#

#

I am currently learning optimization (thanks to Dr. Ravi Varadhan and Dr. Vadim Zipunnikov). Here are some of my thoughts and notes.

<center> <h3>Tools</h3> </center>
 
- [NEOS](https://neos-guide.org/): network enabled optimization system.

- [CVXR](https://cran.r-project.org/web/packages/CVXR/index.html): disciplined convex optimization.

---

<center> <h3>Miscellaneous</h3> </center>

#### 1. Equations systems

If $f(x) = 0$, where $x \in \mathbb{R}^p$, $f \in \mathbb{R}^k$, then

- $k=p$: determined properly

- $k>p$: over determined

- $k<p$: underdetermined (over parametrized)

#### 2. Different convergence rate [More](https://en.wikipedia.org/wiki/Rate_of_convergence)

Consider the series $\{x_k\} \rightarrow x^{*}$

- q linear convergence
$$\text{lim}_{k \rightarrow \infty} \frac{|x_{k+1} - x^*|}{|x_k - x^*|} = c$$
- q super linear convergence
$$\text{lim}_{k \rightarrow \infty} \frac{|x_{k+1} - x^*|}{|x_k - x^*|} \rightarrow 0$$
- q quadratic convergence
$$\text{lim}_{k \rightarrow \infty} \frac{|x_{k+1} - x^*|}{|x_k - x^*|^q} \rightarrow c$$
- q sublinear convergence
$$\text{lim}_{k \rightarrow \infty} \frac{|x_{k+1} - x^*|}{|x_k - x^*|} \rightarrow 1$$

#### 3. Gradient, hessian, and Jacobian

For a scalar function $f: \mathbb{R}^p \rightarrow \mathbb{R}$:

The gradient (first order derivatives) is the vector  $\nabla f = (\frac{\partial f}{\partial x_1}, \cdots,\frac{\partial f}{\partial x_p}) $. 

The hessian (second order derivatives) is the matrix $\nabla^2 f = [\frac{\partial^2 f}{\partial x_i \partial x_j}]$.

For a equation systems $F: \mathbb{R}^p \rightarrow \mathbb{R}^p$:

The Jacobian is the matrix $J = [\frac{\partial F_i}{\partial x_j}]$.

---

<center> <h3>Numeric Derivative</h3> </center>

#### 1.  Standard approach (foward/backward)
$$f'(x) \approx \frac{f(x+h) - f(x)}{h} \approx \frac{f(x) - f(x-h)}{h}$$

#### 2. Centered differecing

$$f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}$$
Derivation based on Talor expansion of $f(x+h)$ and $f(x-h)$.

$$f'(x) = \frac{f(x+h) - f(x-h)}{2h} -\frac{h^2}{12}[f'''(\xi_1) + f'''(\xi_2)]$$

#### 3. Selection of $h$
$$h = \sqrt{\epsilon} \max (|x|,\text{typical } x)$$
where $\epsilon$ is the machine constant.

#### 4. Complex step derivation

$$f(x+ih) \approx f(x) + ihf'(x) $$
$$Img(f(x+ih)) \approx hf'(x)$$
$$f'(x) \approx \frac{1}{h}Img(f(x+ih))$$



#### 5. [More readings](http://www2.math.umd.edu/~dlevy/classes/amsc466/lecture-notes/differentiation-chap.pdf) from Doron Levy.

---

<center> <h3>Newton's Method</h3> </center>

#### 1. Univariate scalar function

$f: \mathbb{R}\rightarrow \mathbb{R}$

$$f(x) = f(x_c) + \int_{x_c}^{x}f'(u)du \approx f(x_c) + f'(x_c)(x-xc)$$

The Newton's estimation is 

$$x_{k+1} = x_{k} - \frac{f(x_k)}{f'(x_k)}$$

#### 2. Equation systems

$F: \mathbb{R}^p\rightarrow \mathbb{R}^p$, and the Jacobian $J = \frac{\partial F_i(x)}{\partial x_j}$

$$F(x_c + p) = F(x_c) + \int_{x_c}^{x}J(t)dt \approx F(x_c) + J(x_c)p$$
The Newton's estimation is 

$$x_{k+1} = x_{k} - F(x_c)J^{-1}(x_c)$$

#### 3. Quasi Newton

The goal is to not calculate $J$ for all the iterations, instead using the Broydan's rank 1 update $x_{k+1} = x_{k} - F(x_c)B_c^{-1}$.

$$B_{k+1} = B_k + \frac{(y - B_k s)s^T}{s^Ts}$$
where $y = F(x_{k+1}) - F(x_k)$, and step $s = x_{k+1} - x_k$. Eventually, the inverse can be done using the sherman morrison formula.

#### 4. Solve Newton's method with Cholesky decomposition

For $f: \mathbb{R}^p \rightarrow \mathbb{R}$, where $f$ is twice differentiable with Llipschitz condition $||\nabla^2f(x) - \nabla^2f(y)||\leq \gamma ||x-y||$, and also $\nabla f(x^*) = 0$, and $\nabla^2 f(x^*)$ is postive definite, the Newtown's method

$$\nabla^2 f(x_k) (x_{k+1}-x_k) = -\nabla f(x_k)$$
can be solved by the Cholesky decomposition $\nabla^2 f(x_k) = LL^T$, where $L$ is a lower triangular matrix by
$$LL^T S_k = -\nabla f(x_k)$$


---

<center> <h3>Gradient Descent</h3> </center>

#### 1. Definition
For a scalar function $f: \mathbb{R}^p \rightarrow \mathbb{R}$, with gradient $\nabla f$, the gradient descent (a.k.a. steepest descent) algorithm is 

$$x_{k+1} = x_k - \lambda \nabla f(x_k)$$

#### 2. Intuition

Gradient descent is based on the observation that if the multi-variable function $f(x)$ is defined and differentiable in a neighborhood of a point $x_c$ , then $f(x)$ decreases fastest if one goes from $x_c$ in the direction of the negative gradient of $f$ at $x_c$, $\nabla f(x_c)$.

#### 3. Line search
The line search approach first finds a descent direction along which the objective function $f$ f will be reduced and then computes a step size that determines how far $x$ should move along that direction, i.e. $\lambda$ varies.


#### 4. Coverngce rate
For gradient descent algorithm 

$$\lim _{k \rightarrow \infty} \frac{|x_{k+1} - x^*|}{|x_k - x^*|} \leq \frac{\lambda_{max} - \lambda_{min}}{\lambda_{max} - \lambda_{min}}$$

where $\lambda_{max}$ and $\lambda_{min}$ are the largest and smallest eigen values of the hessin $\nabla ^2 f(x)$ where $x$ is closed to $x^*$. 
If $\lambda_{max} >> \lambda_{min}$, the $\frac{\lambda_{max} - \lambda_{min}}{\lambda_{max} - \lambda_{min}} \approx 1$, i.e. gradient descent becomes sublinear, and inefficient.


---

<center> <h3>Hybrid Method</h3> </center>

A combination of Newton's method and gradient descent can be formed in a format of a penalized ridge regression 

$$(\nabla^2 f(x_k)+\lambda I)S_k = -\nabla f(x)$$
where $\lambda$ can be $\{\lambda_k\}\rightarrow 0$, e.g. $\lambda = \lambda/10$.



---

<center> <h3>Constrained Optimization</h3> </center>

#### 1. Definition

$$\text{min}_{x \in \Omega  \subseteq \mathbb{R}^p} f(x)$$
where $\Omega$ is th called the feasible set.

#### 2. Types of constraints.

- Box constraints: $l_i \leq x_i \leq u_i$

  We can try to find a transformation $y= T(x)$. E.g. Take $t_i = 2 \frac{x_i - l_i}{u_i - l_i} - 1$, and then $y_i = \log (\frac{1+t_i}{1-t_i})$. And we see that $\Omega_y = \mathbb{R}^p$. But has to be careful that eht solution is not closed to the boundary. Another way is to do the projection. $\text{Proj}(x_i) = \text{median}(l_i, u_i, x_i)$

- Linear equality constraints: $\Omega = \{x|Ax = b\}$

- Linear ineqaulity constraints:$\Omega = \{x|Ax - b \leq 0\}$

- Nonlinear equality or inequality $h_j(x) = 0$ or $g_j(x)\leq 0$

#### 3. Newton's method with constrains.

$\text{min} f(x)$ suich that $Ax = b$.

The lagrangian is  $L(x) = f(x)+ \lambda^T()Ax-b)$, and we want $\nabla L(x) = \nabla f(x) +A^T \lambda = 0$.

The first order optimality condition is ($S_k$ is the current step):

$$\begin{cases} \nabla f(x) +A^T \lambda = 0 \\ AS_k = 0 \end{cases}$$

And we also have

$$\begin{cases} \nabla f(x+S_k) = \nabla f(x) + \nabla^2 f(x)S_k \\  \nabla f(x+S_k) +A^T \lambda = 0\end{cases}$$

Eventually we have the KKT condition which is

$$\begin{cases} \nabla f(x) + \nabla^2 f(x)S_k +A^T \lambda = 0\\ AS_k = 0\end{cases}$$ 
with the matrix form

$$\begin{pmatrix} \nabla^2 f & A^T \\ A & 0 \end{pmatrix} \begin{pmatrix} S_k \\ \lambda\end{pmatrix} = \begin{pmatrix} -\nabla f \\ 0 \end{pmatrix}$$

Solve this to find $\lambda$ and $S_k$. 

#### 4. Optimality on the boundary

```{r, echo=FALSE}
x = seq(1,2, by = 0.01)
y = -2*x^2+4*x+2
plot(x,y,xaxt = "n", yaxt = "n", xlab = "", ylab = "", type = "l",ylim = c(0,6), xlim = c(0,3))
abline(v = c(1,2),lty = 3)
axis(1, at = c(1,2),labels = c("a","b"))
```

Apparently, $f(b)$ is the minimum, but $f'(b) = 0$ fails. Instead, we should check

$$f''(x^*) (x^* -a)(b - x^*) \geq 0$$

#### 5. Reduced hessian

$$[\nabla^2_{\text{R}}f(x)]_{ij} = \begin{cases} \delta_{ij} & \text{   if either i or j constrains is acive} \\ [\nabla^2 f(x)]_{ij} & \text{Otherwise} \end{cases} $$

so that 
$$\nabla^2_{\text{R}}f(x) = \begin{pmatrix} \nabla^2 f(x) & 0 \\ 0 & I \end{pmatrix}$$

is PSD.